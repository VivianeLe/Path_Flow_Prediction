{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD SOLUTION FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from attention import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 1, 25])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def nopeak_mask(size):\n",
    "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
    "     mô hình không nhìn thấy được các từ ở tương lai\n",
    "    \"\"\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    # np_mask = np_mask.to(device)\n",
    "\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg):\n",
    "    \"\"\" Tạo mask cho encoder,\n",
    "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n",
    "    \"\"\"\n",
    "    src_mask = (src != 0).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != 0).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask\n",
    "\n",
    "in_mask, out_mask = create_masks(T_demand, Used_path)\n",
    "in_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract OD demand and path set (X and Y)\n",
    "X: OD demand, graph (link feature), path, link-path adj \\\n",
    "Y: path flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.py\n",
    "%run attention.py\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, size, input_dim0, input_dim1, output_dim0, output_dim1, start_from=0):\n",
    "        super().__init__()\n",
    "        self.path_encoded = path_encoder() # Get path encode dictionary\n",
    "        self.entries = size\n",
    "        self.X = torch.zeros([size, input_dim0, input_dim1], dtype=torch.float32)\n",
    "        self.Y = torch.zeros([size, output_dim0, output_dim1], dtype=torch.float32)\n",
    "\n",
    "        for i in tqdm(range(size)) :\n",
    "            file_name = f\"Output/5by5_Data{start_from+i}\"\n",
    "            x, y = generate_xy(file_name, self.path_encoded)\n",
    "            self.X[i] = x\n",
    "            self.Y[i] = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.entries\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.X[idx]\n",
    "        data_label = self.Y[idx]\n",
    "        return data_point, data_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run helpers.py\n",
    "\n",
    "# Load first 1000 sample data\n",
    "train_size = 10\n",
    "train_dataset = Dataset(train_size, 625, 1165, 625, 3)\n",
    "# this code create batch data, shape batch_size x seq_length x dim\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5ddf873cf54c8cb7129a6af847868a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "size = 30\n",
    "path_encoded = path_encoder()\n",
    "X = torch.zeros([size, 625, 1165], dtype=torch.float32)\n",
    "Y = torch.zeros([size, 625, 3], dtype=torch.float32)\n",
    "for i in tqdm(range(size)) :\n",
    "    file_name = f\"Output/5by5_Data{i}\"\n",
    "    x, y = generate_xy(file_name, path_encoded)\n",
    "    X[i] = x \n",
    "    Y[i] = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 728125])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(X, start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 625, 625])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.matmul(X, X.transpose(-2, -1))/math.sqrt(X.size(-1))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "q = torch.randint(0, 500, (30, 8, 625, 64)).float()\n",
    "k = torch.randint(0, 500, (30, 8, 625, 64)).float()\n",
    "v = torch.randint(0, 500, (30, 8, 625, 64)).float()\n",
    "mask_tensor = mask_tensor.unsqueeze(1)\n",
    "\n",
    "values, attention = scaled_dot_product(q, k, v, mask_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([30, 728125])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1, 1, 728125]}, size=[1, 1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     13\u001b[0m mask_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 14\u001b[0m mask_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mexpand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# mask_tensor = (X!=0).unsqueeze(1)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, mask_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[84], line 7\u001b[0m, in \u001b[0;36mexpand_mask\u001b[1;34m(mask)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand_mask\u001b[39m(mask):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Mở rộng mask trên chiều thứ tư\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m         mask[i] \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[1, 1, 728125]}, size=[1, 1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "%run Transformer1.py\n",
    "%run attention.py\n",
    "\n",
    "def expand_mask(mask):\n",
    "    for i in range(mask.shape[0]):\n",
    "        # Mở rộng mask trên chiều thứ tư\n",
    "        mask[i] = mask[i].expand(mask.shape[2], mask.shape[2])\n",
    "    return mask\n",
    "\n",
    "# print(\"input shape: \", X.shape)\n",
    "X = torch.flatten(X, start_dim=1)\n",
    "print(\"input shape: \", X.shape)\n",
    "mask_tensor = torch.rand(X.shape[0], 1, X.shape[-1]).float()\n",
    "mask_tensor = expand_mask(mask_tensor.unsqueeze(1))\n",
    "# mask_tensor = (X!=0).unsqueeze(1)\n",
    "print(\"Mask shape: \", mask_tensor.shape)\n",
    "\n",
    "# transformer = TransformerEncoder(num_layers=4,input_dim=1165,dim_feedforward=512,num_heads=8, dropout=0.1)\n",
    "# x2 = transformer(X, mask_tensor)\n",
    "# x2 = Encoder(input_dim=X.shape[-1], d_model=512, N=2, heads=8, dropout=0.1)(X, mask_tensor)\n",
    "# print(\"Encoder output shape: \", x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score shape:  torch.Size([30, 8, 625, 625])\n",
      "V shape:  torch.Size([30, 8, 625, 64])\n",
      "Score shape:  torch.Size([30, 8, 625, 625])\n",
      "V shape:  torch.Size([30, 8, 625, 64])\n",
      "Score shape:  torch.Size([30, 8, 625, 625])\n",
      "V shape:  torch.Size([30, 8, 625, 64])\n",
      "Score shape:  torch.Size([30, 8, 625, 625])\n",
      "V shape:  torch.Size([30, 8, 625, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 625, 512])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run attention.py\n",
    "decoder = Decoder(3, 512, 2, 8, 0.1)\n",
    "x3 = decoder(Y, x2, mask_tensor)\n",
    "x3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
