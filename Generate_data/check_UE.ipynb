{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "\n",
    "def read_file(filename):\n",
    "  with open(filename, \"rb\") as file:\n",
    "      stat = pickle.load(file)\n",
    "      file.close()\n",
    "  return stat\n",
    "\n",
    "def get_origin_path(stat):\n",
    "    path_link = stat['data']['paths_link']\n",
    "    od = [k for k in path_link.keys()]\n",
    "    path1 = [tuple(p[0]) for p in path_link.values()]\n",
    "    path2 = [tuple(p[1]) for p in path_link.values()]\n",
    "    path3 = [tuple(p[2]) for p in path_link.values()]\n",
    "\n",
    "    demand_dic = stat[\"data\"][\"demand\"]\n",
    "    demand = [v for v in demand_dic.values()]\n",
    "    path_link_df = pd.DataFrame({\"od\": od, \"demand\":demand, \"path1\": path1, \"path2\": path2, \"path3\": path3})\n",
    "    return path_link_df\n",
    "\n",
    "def get_UE_link_cost(stat):\n",
    "    # return a dataframe of link cost, link flow\n",
    "    link = stat['data']['network'].copy()\n",
    "    link['link_flow'] = stat['link_flow']\n",
    "    # Calculate link cost\n",
    "    link['link_cost'] = round(link['free_flow_time']*\\\n",
    "                            (1+link['b']*((link['link_flow']/link['capacity'])**1)), 2)\n",
    "    return link\n",
    "\n",
    "# Calculate path travel time for each od pair\n",
    "def calculate_path_cost(row, link_df):\n",
    "    sum_time = 0\n",
    "    for l in row:\n",
    "        sum_time += link_df.at[l, 'link_cost']\n",
    "    return round(sum_time, 2)\n",
    "\n",
    "# calculate each link flow based on path flow\n",
    "def extract_link_flow(path_link, flows):\n",
    "    # input: a dictionary of {od pair: path_link} and list of flow distribution\n",
    "    # return a dictionary of link flow\n",
    "    path_flow = {}\n",
    "    for path_set, flow_set in zip(path_link.values(), flows):\n",
    "        for path, flow in zip(path_set, flow_set):\n",
    "            path_flow[tuple(path)] = flow\n",
    "\n",
    "    aggregated_sums = defaultdict(float)\n",
    "    for path, flow in path_flow.items():\n",
    "        for link in path:\n",
    "            aggregated_sums[link] += flow\n",
    "    link_flow = dict(aggregated_sums)\n",
    "    return link_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check UE of origin dataset \n",
    "def mean_path_cost(filename):\n",
    "    stat = read_file(filename)\n",
    "    path_link_df = get_origin_path(stat)\n",
    "    UE_link = get_UE_link_cost(stat)\n",
    "\n",
    "    path_link_df['path1_cost'] = path_link_df['path1'].apply(lambda x: calculate_path_cost(x, UE_link))\n",
    "    path_link_df['path2_cost'] = path_link_df['path2'].apply(lambda x: calculate_path_cost(x, UE_link))\n",
    "    path_link_df['path3_cost'] = path_link_df['path3'].apply(lambda x: calculate_path_cost(x, UE_link))\n",
    "\n",
    "    flows = stat['path_flow']\n",
    "    path_link_df['flow1'] = [f[0] for f in flows]\n",
    "    path_link_df['flow2'] = [f[1] for f in flows]\n",
    "    path_link_df['flow3'] = [f[2] for f in flows]\n",
    "\n",
    "    mean_path_cost = (np.mean(path_link_df['path1_cost']) + np.mean(path_link_df['path2_cost']) + np.mean(path_link_df['path3_cost']))/3\n",
    "    return path_link_df, mean_path_cost\n",
    "# path_link_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average path cost\n",
    "sum_avg = 0\n",
    "size = 10000\n",
    "for i in range(size):\n",
    "    file_name = f\"../Output/5by5_Data{i}\"\n",
    "    df, mean_cost = mean_path_cost(file_name)\n",
    "    sum_avg += mean_cost\n",
    "avg = sum_avg/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9182720531969593"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def load_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = np.loadtxt(f)\n",
    "    num_tensors = data.size // (3 * 625)\n",
    "    reshaped_data = data.reshape((num_tensors, 3, 625))\n",
    "    tensors = [tf.convert_to_tensor(reshaped_data[i], dtype=tf.float32) for i in range(num_tensors)]\n",
    "    return tensors\n",
    "\n",
    "predicted_values = load_from_file('../predicted_values.txt')\n",
    "print(len(predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flow(tensor):\n",
    "  # input: a single tensor of predicted path flow\n",
    "  # return a dictionary of full information: od pair,predicted path flow\n",
    "  final_dict = {}\n",
    "  for sub_flow_index, sub_flow in enumerate(tensor):\n",
    "    sub_flow_dict = {(i+1, j+1): int(sub_flow[i, j]) for i in range(sub_flow.shape[0]) for j in range(sub_flow.shape[1])}\n",
    "\n",
    "    for key, value in sub_flow_dict.items():\n",
    "        if key not in final_dict:\n",
    "            final_dict[key] = [None] * tensor.shape[0]\n",
    "        final_dict[key][sub_flow_index] = value\n",
    "  final_dict = {k: v for k, v in final_dict.items() if not all(val == 0 for val in v)}\n",
    "  return final_dict\n",
    "\n",
    "def create_pred_df(tensor, stat):\n",
    "  final_dict = extract_flow(tensor)\n",
    "  print(\"Number of OD pairs predicted: \", len(final_dict))\n",
    "  print(\"Number of origin OD pairs: \", len(stat['path_flow']))\n",
    "  \n",
    "  flow_df = pd.DataFrame.from_dict(final_dict, orient='index', columns=['pred_f1', 'pred_f2', 'pred_f3']).reset_index()\n",
    "  flow_df.rename(columns={'index': 'od'}, inplace=True)\n",
    "  pred_df = get_origin_path(stat)[['od', 'demand', 'path1', 'path2', 'path3']]\n",
    "  pred_df = pd.merge(pred_df, flow_df, how='left', on='od')\n",
    "  nan_val = pred_df['pred_f1'].isna().sum()\n",
    "  # Percentage of nan value\n",
    "  print(\"Nan values: \", nan_val, \" -- \", round(nan_val/len(stat['path_flow'])*100,2), \"%\")\n",
    "  pred_df = pred_df.fillna(0)\n",
    "  return pred_df\n",
    "\n",
    "# Calculate link flow from pred path flow\n",
    "def sum_pred_link_flow(pred_df, stat):\n",
    "    pred_path_flow = pred_df[['pred_f1', 'pred_f2', 'pred_f3']].values.tolist()\n",
    "    path_link = stat['data']['paths_link']\n",
    "\n",
    "    pred_link_flow = extract_link_flow(path_link, pred_path_flow)\n",
    "    pred_link_flow = pd.DataFrame.from_dict(pred_link_flow, orient='index', columns=['pred_link_flow']).sort_index(ascending=True).reset_index()\n",
    "    pred_link_flow.rename(columns={'index': 'link_id'}, inplace=True)\n",
    "    link = stat['data']['network'].copy()[['link_id', 'capacity', 'free_flow_time', 'b']]\n",
    "    output = pd.merge(link, pred_link_flow, how='left', on='link_id')\n",
    "    output = output.fillna(0)\n",
    "    output['link_cost'] = round(output['free_flow_time']*\\\n",
    "                            (1+output['b']*((output['pred_link_flow']/output['capacity'])**1)), 2)\n",
    "    return output\n",
    "\n",
    "def calculate_delay(pred_df, pred_link_flow):\n",
    "    pred_df['path1_cost'] = pred_df['path1'].apply(lambda x: calculate_path_cost(x, pred_link_flow))\n",
    "    pred_df['path2_cost'] = pred_df['path2'].apply(lambda x: calculate_path_cost(x, pred_link_flow))\n",
    "    pred_df['path3_cost'] = pred_df['path3'].apply(lambda x: calculate_path_cost(x, pred_link_flow))\n",
    "    pred_df['min_path_cost'] = pred_df[['path1_cost', 'path2_cost', 'path3_cost']].min(axis=1)\n",
    "    pred_df['delay'] = (\n",
    "        pred_df['pred_f1'] * (pred_df['path1_cost'] - pred_df['min_path_cost']) +\n",
    "        pred_df['pred_f2'] * (pred_df['path2_cost'] - pred_df['min_path_cost']) +\n",
    "        pred_df['pred_f3'] * (pred_df['path3_cost'] - pred_df['min_path_cost'])\n",
    "    )\n",
    "    avg_delay = pred_df['delay'].sum()/pred_df['demand'].sum()\n",
    "    #return average delay in minutes\n",
    "    return avg_delay*60\n",
    "\n",
    "def single_avg_delay(pred_tensor, filename):\n",
    "    stat = read_file(filename)\n",
    "    a = tf.reshape(pred_tensor, (3, 25, 25))\n",
    "    pred_df = create_pred_df(a, stat)\n",
    "    pred_link_flow = sum_pred_link_flow(pred_df, stat)\n",
    "    avg_delay = calculate_delay(pred_df, pred_link_flow)\n",
    "    return avg_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../parameters.py\n",
    "p = Params()\n",
    "\n",
    "# Check number of OD pair in origin dataset \n",
    "start_from=1800\n",
    "files = []\n",
    "for i in range(10):\n",
    "    file_name = ''.join([p.base_path, str(start_from+i)])\n",
    "    files.append(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OD pairs predicted:  322\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  54  --  42.52 %\n",
      "Average delay: 3.817 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  317\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  54  --  42.52 %\n",
      "Average delay: 4.133 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  311\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  51  --  40.16 %\n",
      "Average delay: 1.65 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  315\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  54  --  42.52 %\n",
      "Average delay: 3.999 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  318\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  58  --  45.67 %\n",
      "Average delay: 3.455 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  317\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  60  --  47.24 %\n",
      "Average delay: 3.42 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  322\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  43  --  33.86 %\n",
      "Average delay: 6.932 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  313\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  49  --  38.58 %\n",
      "Average delay: 1.992 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  313\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  61  --  48.03 %\n",
      "Average delay: 5.084 mins\n",
      "-----------------------------\n",
      "Number of OD pairs predicted:  312\n",
      "Number of origin OD pairs:  127\n",
      "Nan values:  60  --  47.24 %\n",
      "Average delay: 1.74 mins\n",
      "-----------------------------\n",
      "avg delay of prediction:  3.6222227651639507\n"
     ]
    }
   ],
   "source": [
    "# Check len of predicted path flow distribution\n",
    "filename = '../Output/5by5_Data1800'\n",
    "\n",
    "sum_delay = 0\n",
    "for i, filename in zip(predicted_values, files):\n",
    "    avg_delay = single_avg_delay(i, filename)\n",
    "    sum_delay += avg_delay\n",
    "    print(f\"Average delay: {round(avg_delay,3)} mins\")\n",
    "    print(\"-----------------------------\")\n",
    "print(\"avg delay of prediction: \", sum_delay/len(predicted_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
