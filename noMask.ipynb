{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers as tfl\n",
    "from tqdm.notebook import tqdm\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.mixed_precision import set_global_policy, Policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "class EncoderLayer(tfl.Layer):\n",
    "    def __init__(self, input_dim, d_model, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(d_model, activation='relu', kernel_regularizer=l2(0.1))\n",
    "        self.attn_layer = tfl.MultiHeadAttention(num_heads=heads, key_dim=d_model // heads, dropout=dropout)\n",
    "        self.layer_norm1 = tfl.LayerNormalization(epsilon=1e-6)\n",
    "        self.dense2 = tfl.Dense(input_dim)\n",
    "        self.dropout = tfl.Dropout(dropout)\n",
    "        self.layer_norm2 = tfl.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        attn_output = self.attn_layer(query=x, key=x, value=x, training=training)\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output), training=training)\n",
    "        ffn_output = self.dense2(self.dropout(self.dense1(x)), training=training)\n",
    "        x = self.layer_norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "class Encoder(tfl.Layer):\n",
    "    def __init__(self,input_dim, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for _ in range(N):\n",
    "            self.layers.append(EncoderLayer(input_dim, d_model, heads, dropout))\n",
    "        self.dense = tfl.Dense(3, activation='relu', kernel_regularizer=l2(0.1))\n",
    "    def call(self, x, training=None):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, training=training)\n",
    "        return output\n",
    "\n",
    "class DecoderLayer(tfl.Layer):\n",
    "    def __init__(self, output_dim, d_model, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mha1 = tfl.MultiHeadAttention(num_heads=heads, key_dim=d_model // heads, dropout=dropout, attention_axes=(2))\n",
    "        self.layer_norm1 = tfl.LayerNormalization(epsilon=1e-6)\n",
    "        self.mha2 = tfl.MultiHeadAttention(num_heads=heads, key_dim=d_model // heads, dropout=dropout, attention_axes=(2))\n",
    "        self.layer_norm2 = tfl.LayerNormalization(epsilon=1e-6)\n",
    "        self.dense1 = tfl.Dense(d_model, activation='relu', kernel_regularizer=l2(0.1))\n",
    "        self.dense2 = tfl.Dense(output_dim)\n",
    "        self.layer_norm3 = tfl.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tfl.Dropout(dropout)\n",
    "        self.dropout2 = tfl.Dropout(dropout)\n",
    "        self.dropout3 = tfl.Dropout(dropout)\n",
    "    def call(self, x, encoder_output, training=None):\n",
    "        attn1 = self.mha1(query=x, key=x, value=x, training=training)\n",
    "        x = self.layer_norm1(x + self.dropout1(attn1), training=training)\n",
    "        # Encoder-decoder attention\n",
    "        attn2 = self.mha2(query=x, key=encoder_output, value=encoder_output, training=training)\n",
    "        x = self.layer_norm2(x + self.dropout2(attn2), training=training)\n",
    "        ffn_output = self.dense2(self.dropout3(self.dense1(x)), training=training)\n",
    "        x = self.layer_norm3(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "class Decoder(tfl.Layer):\n",
    "    def __init__(self, output_dim, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for _ in range(N):\n",
    "            self.layers.append(DecoderLayer(output_dim, d_model, heads, dropout))\n",
    "        self.layer_norm = tfl.LayerNormalization()\n",
    "    def call(self, x, encoder_output, training = None):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, encoder_output, training=training)\n",
    "        output = self.layer_norm(output)\n",
    "        return output\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(output_dim, d_model, N, heads, dropout)\n",
    "        self.activate = tfl.Activation(\"sigmoid\")\n",
    "    def call(self, x, y, training=None):\n",
    "        encoder_output = self.encoder(x, training=training)\n",
    "        decoder_output = self.decoder(y, encoder_output, training=training)\n",
    "        decoder_output = self.activate(decoder_output)\n",
    "        return decoder_output\n",
    "\n",
    "    def eval(self):\n",
    "        for layer in self.encoder.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in self.decoder.layers:\n",
    "            layer.trainable = False\n",
    "    def train(self):\n",
    "        for layer in self.encoder.layers:\n",
    "            layer.trainable = True\n",
    "        for layer in self.decoder.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "def training_loop(model, train_data_loader, val_data_loader, epochs, loss_fn, optimizer, device, gradient_accumulation_steps=8):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-4)\n",
    "\n",
    "    with tqdm(total=epochs, unit=\"epoch\") as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            total_train_loss = 0\n",
    "            accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "            for step, (src, trg) in enumerate(train_data_loader):\n",
    "                with tf.device(device):\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        output = model(src, trg, training=True)\n",
    "                        loss = loss_fn(trg, output)\n",
    "                        loss = loss / gradient_accumulation_steps  # Normalize the loss to account for gradient accumulation\n",
    "\n",
    "                    # Accumulate gradients\n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    for i, (accum_grad, grad) in enumerate(zip(accumulated_gradients, gradients)):\n",
    "                        if grad is not None:\n",
    "                            accumulated_gradients[i] += grad\n",
    "\n",
    "                    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                        # Remove pair gradient-None\n",
    "                        gradients_and_vars = [\n",
    "                            (grad, var) for grad, var in zip(accumulated_gradients, model.trainable_variables) if grad is not None\n",
    "                        ]\n",
    "                        if gradients_and_vars:\n",
    "                            optimizer.apply_gradients(gradients_and_vars)\n",
    "                        accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "\n",
    "                    total_train_loss += loss.numpy() * gradient_accumulation_steps\n",
    "                    pbar.set_description(f\"Train Loss: {total_train_loss / ((step + 1) * gradient_accumulation_steps):.4f}\")\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            for src, trg in val_data_loader:\n",
    "                # Move the batch to the device\n",
    "                with tf.device(device):\n",
    "                    # Forward pass\n",
    "                    output = model(src, trg, training=False)\n",
    "                    loss = loss_fn(trg, output)\n",
    "                    total_val_loss += loss.numpy()\n",
    "                    pbar.set_description(f\"Val Loss: {total_val_loss / len(val_data_loader):.4f}\")\n",
    "\n",
    "            pbar.update(1)\n",
    "            val_losses.append(total_val_loss / len(val_data_loader))\n",
    "            train_losses.append(total_train_loss / ((step + 1) * gradient_accumulation_steps))\n",
    "            print(f\"Epoch: {epoch+1} - Train Loss: {total_train_loss/((step + 1) * gradient_accumulation_steps):.4f}, \"\n",
    "                  f\"Val Loss: {total_val_loss/len(val_data_loader):.4f}\")\n",
    "\n",
    "            early_stopping.on_epoch_end(epoch, logs={'val_loss': total_val_loss / len(val_data_loader)})\n",
    "            reduce_lr.on_epoch_end(epoch, logs={'val_loss': total_val_loss / len(val_data_loader)})\n",
    "\n",
    "            if early_stopping.stopped_epoch > 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        return model, train_losses, val_losses\n",
    "\n",
    "def predict_withScaler(model, test_data_loader, scalers, device):\n",
    "    model.eval()\n",
    "    predicted_values = []\n",
    "    scaler_idx = 0\n",
    "    for src, trg in test_data_loader:\n",
    "        with tf.device(device):\n",
    "            # output = model.predict(src, src_mask, tgt_mask)\n",
    "            output = model.call(src, trg)\n",
    "            for i in range(len(src)):\n",
    "                scaler = scalers[scaler_idx]\n",
    "                scaler_idx +=1\n",
    "                # pred_matrix = inversed(output[i].numpy(), scaler) # reverse transform by each row\n",
    "                pred_matrix = scaler.inverse_transform(output[i].numpy()) # inverse transform by column\n",
    "                predicted_values.append(pred_matrix)\n",
    "\n",
    "    return predicted_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def load_files_from_folders(folder, max_files):\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    file_list = []\n",
    "    for i, file in enumerate(files):\n",
    "        file_path = os.path.join(folder, file)\n",
    "        if os.path.isfile(file_path) and i < max_files and not file.startswith('.'):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "def read_file(filename):\n",
    "  with open(filename, \"rb\") as file:\n",
    "      stat = pickle.load(file)\n",
    "      file.close()\n",
    "  return stat\n",
    "\n",
    "def split_dataset(files, train_ratio, val_ratio):\n",
    "    random.shuffle(files)\n",
    "\n",
    "    total_files = len(files)\n",
    "    train_size = int(total_files * train_ratio)\n",
    "    val_size = int(total_files * val_ratio)\n",
    "\n",
    "    train_files = files[:train_size]\n",
    "    val_files = files[train_size:train_size + val_size]\n",
    "    test_files = files[train_size + val_size:]\n",
    "\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# Need to load all files in dataset to get unique path dict\n",
    "def path_encoder(files):\n",
    "    path_sample = []\n",
    "    for file_name in files:\n",
    "        stat = read_file(file_name)\n",
    "        path_sample.append(stat[\"data\"][\"paths_link\"])\n",
    "\n",
    "    all_path_link = [path_sample[i].values() for i in range(len(path_sample))]\n",
    "    unique_values_set = {tuple(p) for path_set in all_path_link for path in path_set for p in path}\n",
    "    path_set_dict = {v: k for k, v in enumerate(unique_values_set, start=1)}\n",
    "    return path_set_dict\n",
    "\n",
    "# path_encoded = path_encoder()\n",
    "\n",
    "def normalize(tensor):\n",
    "    scaler = MinMaxScaler()\n",
    "    normed = scaler.fit_transform(tensor)\n",
    "    return normed\n",
    "\n",
    "def normalizeY(tensor):\n",
    "    scaler = MinMaxScaler()\n",
    "    normed = scaler.fit_transform(tensor)\n",
    "    return normed, scaler\n",
    "\n",
    "# def normalizeY(tensor):\n",
    "#     # Normalize by row\n",
    "#     if not isinstance(tensor, np.ndarray):\n",
    "#         tensor = np.array(tensor)\n",
    "#     scaler = MinMaxScaler()\n",
    "#     normed = scaler.fit_transform(np.transpose(tensor))\n",
    "#     tensor = np.transpose(normed)\n",
    "#     tensor = tensor[:, :-1] # get 1st 3 columns, ignore the last column of demand\n",
    "#     return tensor, scaler\n",
    "\n",
    "def create_matrix(data, nodes):\n",
    "    # data is an array, nodes is a set\n",
    "    matrix = np.zeros((len(nodes), len(nodes)))\n",
    "    # matrix = np.zeros((24, 24))\n",
    "    for (o, d), v in data:\n",
    "        o = int(o)\n",
    "        d = int(d)\n",
    "        matrix[o-1][d-1] = v\n",
    "    matrix = matrix.reshape(-1, 1).astype(float) # 625x1\n",
    "    return matrix\n",
    "\n",
    "def get_graphMatrix(network, nodes):\n",
    "    # 625x3\n",
    "    cap = np.array(network[['init_node', 'term_node', 'capacity']].apply(lambda row: ((row['init_node'], row['term_node']), row['capacity']), axis=1).tolist(), dtype=object)\n",
    "    length = np.array(network[['init_node', 'term_node', 'length']].apply(lambda row: ((row['init_node'], row['term_node']), row['length']), axis=1).tolist(), dtype=object)\n",
    "    fft = np.array(network[['init_node', 'term_node', 'free_flow_time']].apply(lambda row: ((row['init_node'], row['term_node']), row['free_flow_time']), axis=1).tolist(), dtype=object)\n",
    "\n",
    "    # Cap = create_matrix(cap, nodes)\n",
    "    Length = create_matrix(length, nodes)\n",
    "    Fft = create_matrix(fft, nodes)\n",
    "\n",
    "    # matrix = np.concatenate((normalize(Cap), np.log1p(Length), np.log1p(Fft)), axis=1)\n",
    "    matrix = np.concatenate((np.log1p(Length), np.log1p(Fft)), axis=1)\n",
    "    return matrix\n",
    "\n",
    "def get_demandMatrix(demand, nodes):\n",
    "    # 625x1\n",
    "    tensor = np.array([(key, value) for key, value in demand.items()], dtype=object)\n",
    "    tensor = create_matrix(tensor, nodes)\n",
    "    return tensor\n",
    "\n",
    "# Get 3 feasible paths for each OD pair, return tensor shape 625x3\n",
    "def get_pathMatrix(path_links, nodes, unique_set):\n",
    "    # 625x3\n",
    "    paths = np.array([(key, [tuple(path) for path in value]) for key, value in path_links.items()], dtype=object)\n",
    "    p1, p2, p3 = [], [], []\n",
    "    for od, path_list in paths:\n",
    "        path1 = path2 = path3 = 0\n",
    "\n",
    "        if len(path_list) > 0:\n",
    "            path1 = path_list[0]\n",
    "        if len(path_list) > 1:\n",
    "            path2 = path_list[1]\n",
    "        if len(path_list) > 2:\n",
    "            path3 = path_list[2]\n",
    "\n",
    "        p1.append((od, unique_set[path1] if path1 != 0 else 0))\n",
    "        p2.append((od, unique_set[path2] if path2 != 0 else 0))\n",
    "        p3.append((od, unique_set[path3] if path3 != 0 else 0))\n",
    "    p1 = create_matrix(p1, nodes)\n",
    "    p2 = create_matrix(p2, nodes)\n",
    "    p3 = create_matrix(p3, nodes)\n",
    "    matrix = np.concatenate((p1, p2, p3), axis=1)\n",
    "    return matrix\n",
    "\n",
    "# Get path flow distribution (Y), return a tensor 625x3\n",
    "def get_flowMatrix(demand, path_flows, nodes):\n",
    "    # 625x3\n",
    "    flows = np.array([(k, v) for k, v in zip(demand.keys(), path_flows)], dtype=object)\n",
    "    p1, p2, p3 = [], [], []\n",
    "    for od, flow in flows:\n",
    "        path1 = path2 = path3 = 0\n",
    "        if len(flow) > 0:\n",
    "            path1 = flow[0]\n",
    "        if len(flow) > 1:\n",
    "            path2 = flow[1]\n",
    "        if len(flow) > 2:\n",
    "            path3 = flow[2]\n",
    "\n",
    "        p1.append((od, path1 if path1 != 0 else 0))\n",
    "        p2.append((od, path2 if path2 != 0 else 0))\n",
    "        p3.append((od, path3 if path3 != 0 else 0))\n",
    "    p1 = create_matrix(p1, nodes)\n",
    "    p2 = create_matrix(p2, nodes)\n",
    "    p3 = create_matrix(p3, nodes)\n",
    "\n",
    "    matrix = np.concatenate((p1, p2, p3), axis=1)\n",
    "    return matrix\n",
    "\n",
    "def to_percentage_list(lst):\n",
    "    total = sum(lst)\n",
    "    if total == 0:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    return [x / total for x in lst]\n",
    "\n",
    "# No mask model\n",
    "def generate_xy(file_name, unique_set, test_set=None):\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        stat = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    path_links = stat[\"data\"][\"paths_link\"]\n",
    "    demand = stat[\"data\"][\"demand\"]\n",
    "    path_flows = stat[\"path_flow\"]\n",
    "    path_flows = [to_percentage_list(inner_list) for inner_list in path_flows]\n",
    "    nodes = stat[\"data\"][\"nodes\"]\n",
    "    net = stat[\"data\"][\"network\"]\n",
    "\n",
    "    # Get X\n",
    "    Graph = get_graphMatrix(net, nodes) #return normalized data\n",
    "    OD_demand = get_demandMatrix(demand, nodes)\n",
    "    Path_tensor = get_pathMatrix(path_links, nodes, unique_set)\n",
    "\n",
    "    X = np.concatenate((Graph, normalize(OD_demand), normalize(Path_tensor)), axis=1)\n",
    "    # X = normalize(X)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32) # 625x6\n",
    "\n",
    "    # Get Y\n",
    "    Y = get_flowMatrix(demand, path_flows, nodes)\n",
    "    # Y = np.concatenate((Y, OD_demand), axis=1)\n",
    "    Y, scaler = normalizeY(Y)\n",
    "    Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "    if test_set:\n",
    "        return X, Y, scaler\n",
    "    return X, Y\n",
    "\n",
    "\"\"\"\n",
    "CHECK UE CONDITIONS OF PREDICTED OUTPUT\n",
    "\"\"\"\n",
    "\n",
    "def get_origin_path(stat):\n",
    "    path_link = stat['data']['paths_link']\n",
    "    od = [k for k in path_link.keys()]\n",
    "    path1 = [tuple(p[0]) if len(p) > 0 else np.nan for p in path_link.values()]\n",
    "    path2 = [tuple(p[1]) if len(p) > 1 else np.nan for p in path_link.values()]\n",
    "    path3 = [tuple(p[2]) if len(p) > 2 else np.nan for p in path_link.values()]\n",
    "\n",
    "    demand_dic = stat[\"data\"][\"demand\"]\n",
    "    demand = [v for v in demand_dic.values()]\n",
    "    path_link_df = pd.DataFrame({\"od\": od, \"demand\":demand, \"path1\": path1, \"path2\": path2, \"path3\": path3})\n",
    "    return path_link_df\n",
    "\n",
    "def get_UE_link_cost(stat):\n",
    "    # return a dataframe of link cost, link flow\n",
    "    link = stat['data']['network'].copy()\n",
    "    link['link_flow'] = stat['link_flow']\n",
    "    # Calculate link cost\n",
    "    link['link_cost'] = round(link['free_flow_time']*\\\n",
    "                            (1+link['b']*((link['link_flow']/link['capacity'])**4)), 2)\n",
    "    return link\n",
    "\n",
    "# Calculate path travel time for each od pair\n",
    "def calculate_path_cost(row, link_df):\n",
    "    if pd.isna(row): \n",
    "        return np.nan\n",
    "    if row == 0:\n",
    "        return np.nan\n",
    "\n",
    "    sum_time = 0\n",
    "    for link in row:\n",
    "        # sum_time += link_df[link_df['link_id']==link]['link_cost'].iloc[0]\n",
    "        sum_time += link_df.at[link, 'link_cost']\n",
    "    return round(sum_time, 2)\n",
    "\n",
    "# calculate each link flow based on path flow\n",
    "def extract_link_flow(path_link, flows):\n",
    "    # input: a dictionary of {od pair: path_link} and list of flow distribution\n",
    "    # return a dictionary of link flow\n",
    "    path_flow = {}\n",
    "    for path_set, flow_set in zip(path_link.values(), flows):\n",
    "        for path, flow in zip(path_set, flow_set):\n",
    "            path_flow[tuple(path)] = flow\n",
    "\n",
    "    aggregated_sums = defaultdict(float)\n",
    "    for path, flow in path_flow.items():\n",
    "        for link in path:\n",
    "            aggregated_sums[link] += flow\n",
    "    link_flow = dict(aggregated_sums)\n",
    "    return link_flow\n",
    "\n",
    "def extract_flow(pred_tensor):\n",
    "    x = int(np.sqrt(pred_tensor.shape[0]))\n",
    "    pred1 = pred_tensor[:, 0].reshape(x, x)\n",
    "    pred2 = pred_tensor[:, 1].reshape(x, x)\n",
    "    pred3 = pred_tensor[:, 2].reshape(x, x)\n",
    "\n",
    "    dict1 = {(i+1, j+1): pred1[i, j] for i in range(pred1.shape[0]) for j in range(pred1.shape[1])}\n",
    "    dict2 = {(i+1, j+1): pred2[i, j] for i in range(pred2.shape[0]) for j in range(pred2.shape[1])}\n",
    "    dict3 = {(i+1, j+1): pred3[i, j] for i in range(pred3.shape[0]) for j in range(pred3.shape[1])}\n",
    "\n",
    "    final_dict = {}\n",
    "    for key in dict1.keys():\n",
    "        final_dict[key] = [dict1[key], dict2[key], dict3[key]]\n",
    "    final_dict = {k: v for k, v in final_dict.items() if not all(val == 0 for val in v)}\n",
    "    return final_dict\n",
    "\n",
    "def create_pred_df(tensor, stat):\n",
    "    final_dict = extract_flow(tensor)\n",
    "    flow_df = pd.DataFrame.from_dict(final_dict, orient='index', columns=['flow1', 'flow2', 'flow3']).reset_index()\n",
    "    flow_df.rename(columns={'index': 'od'}, inplace=True)\n",
    "    pred_df = get_origin_path(stat)[['od', 'demand', 'path1', 'path2', 'path3']]\n",
    "    pred_df = pd.merge(pred_df, flow_df, how='left', on='od')\n",
    "    nan_val = pred_df['flow1'].isna().sum()\n",
    "    nan_num = round(nan_val/len(stat['path_flow']),2)\n",
    "    pred_df = pred_df.fillna(0)\n",
    "    pred_df.loc[pred_df['path1'] == 0, 'flow1'] = 0\n",
    "    pred_df.loc[pred_df['path2'] == 0, 'flow2'] = 0\n",
    "    pred_df.loc[pred_df['path3'] == 0, 'flow3'] = 0\n",
    "\n",
    "    pred_df.loc[pred_df['flow1'] < 0, 'flow1'] = 0\n",
    "    pred_df.loc[pred_df['flow2'] < 0, 'flow2'] = 0\n",
    "    pred_df.loc[pred_df['flow3'] < 0, 'flow3'] = 0\n",
    "\n",
    "    pred_df['flow1'] = round(pred_df['flow1'], 0)\n",
    "    pred_df['flow2'] = round(pred_df['flow2'], 0)\n",
    "    pred_df['flow3'] = round(pred_df['flow3'], 0)\n",
    "\n",
    "    pred_df['flow1'] = pred_df['flow1'] * pred_df['demand']\n",
    "    pred_df['flow2'] = pred_df['flow2'] * pred_df['demand']\n",
    "    pred_df['flow3'] = pred_df['flow3'] * pred_df['demand']\n",
    "\n",
    "    return pred_df, len(stat['path_flow']), len(final_dict), nan_num\n",
    "\n",
    "def sum_pred_link_flow(pred_df, stat):\n",
    "    pred_path_flow = pred_df[['flow1', 'flow2', 'flow3']].values.tolist()\n",
    "    path_link = stat['data']['paths_link']\n",
    "\n",
    "    pred_link_flow = extract_link_flow(path_link, pred_path_flow)\n",
    "    pred_link_flow = pd.DataFrame.from_dict(pred_link_flow, orient='index', columns=['link_flow']).sort_index(ascending=True).reset_index()\n",
    "    pred_link_flow.rename(columns={'index': 'link_id'}, inplace=True)\n",
    "    link = stat['data']['network'].copy()[['link_id', 'capacity', 'free_flow_time', 'b']]\n",
    "    output = pd.merge(link, pred_link_flow, how='left', on='link_id')\n",
    "    output = output.fillna(0)\n",
    "    output['link_cost'] = round(output['free_flow_time']*\\\n",
    "                            (1+output['b']*((output['link_flow']/output['capacity'])**4)), 2)\n",
    "    return output\n",
    "\n",
    "def calculate_delay(pred_df, pred_link_flow):\n",
    "    pred_df['path1_cost'] = pred_df['path1'].apply(lambda x: calculate_path_cost(x, pred_link_flow))\n",
    "    pred_df['path2_cost'] = pred_df['path2'].apply(lambda x: calculate_path_cost(x, pred_link_flow))\n",
    "    pred_df['path3_cost'] = pred_df['path3'].apply(lambda x: calculate_path_cost(x, pred_link_flow))\n",
    "    pred_df['min_path_cost'] = pred_df[['path1_cost', 'path2_cost', 'path3_cost']].min(axis=1)\n",
    "    pred_df = pred_df.fillna(0)\n",
    "    pred_df['delay'] = (\n",
    "        pred_df['flow1'] * (pred_df['path1_cost'] - pred_df['min_path_cost']) +\n",
    "        pred_df['flow2'] * (pred_df['path2_cost'] - pred_df['min_path_cost']) +\n",
    "        pred_df['flow3'] * (pred_df['path3_cost'] - pred_df['min_path_cost'])\n",
    "    )\n",
    "    avg_delay = pred_df['delay'].sum()/pred_df['demand'].sum()\n",
    "    #return average delay in minutes\n",
    "    return pred_df, avg_delay\n",
    "\n",
    "def mean_path_cost(stat):\n",
    "    path_link_df = get_origin_path(stat)\n",
    "    UE_link = get_UE_link_cost(stat)\n",
    "\n",
    "    path_link_df['path1_cost'] = path_link_df['path1'].apply(lambda x: calculate_path_cost(x, UE_link))\n",
    "    path_link_df['path2_cost'] = path_link_df['path2'].apply(lambda x: calculate_path_cost(x, UE_link))\n",
    "    path_link_df['path3_cost'] = path_link_df['path3'].apply(lambda x: calculate_path_cost(x, UE_link))\n",
    "\n",
    "    flows = stat['path_flow']\n",
    "    p1, p2, p3 = [], [], []\n",
    "    for flow in flows:\n",
    "        path1 = path2 = path3 = 0\n",
    "        if len(flow) > 0:\n",
    "            path1 = flow[0]\n",
    "        if len(flow) > 1:\n",
    "            path2 = flow[1]\n",
    "        if len(flow) > 2:\n",
    "            path3 = flow[2]\n",
    "\n",
    "        p1.append((path1 if path1 != 0 else 0))\n",
    "        p2.append((path2 if path2 != 0 else 0))\n",
    "        p3.append((path3 if path3 != 0 else 0))\n",
    "    path_link_df['flow1'] = p1\n",
    "    path_link_df['flow2'] = p2\n",
    "    path_link_df['flow3'] = p3\n",
    "\n",
    "    avg_path_cost = (np.mean(path_link_df['path1_cost']) + np.mean(path_link_df['path2_cost']) + np.mean(path_link_df['path3_cost']))/3\n",
    "    return UE_link, path_link_df, avg_path_cost\n",
    "\n",
    "def compare_link_flow(UE_link, pred_link_flow):\n",
    "    # Calculate abs err and sqr err of link flow\n",
    "    UE_link = UE_link[['link_id', 'link_flow']]\n",
    "    UE_link = UE_link.rename(columns={'link_flow': 'UE_flow'})\n",
    "    link_flow = pd.merge(pred_link_flow, UE_link, on='link_id', how='right')\n",
    "    link_flow = link_flow.drop(['capacity','free_flow_time', 'b', 'link_cost'], axis=1)\n",
    "    link_flow['abs_err'] = (link_flow['link_flow'] - link_flow['UE_flow']).abs()\n",
    "    link_flow['sqr_err'] = link_flow['abs_err']**2\n",
    "    return link_flow\n",
    "\n",
    "def get_all_path_flow(df):\n",
    "    # Transform the table of 6 columns to 2 columns\n",
    "    path_df = pd.melt(df, value_vars=['path1', 'path2', 'path3'], var_name='path_type', value_name='path')\n",
    "    flow_df = pd.melt(df, value_vars=['flow1', 'flow2', 'flow3'], var_name='flow_type', value_name='flow')\n",
    "    result_df = pd.concat([path_df['path'], flow_df['flow']], axis=1)\n",
    "    return result_df\n",
    "\n",
    "def compare_path_flow(path_link_df, pred_df):\n",
    "    # Calculate abs err and sqr err of path flow\n",
    "    pred_path_flow = get_all_path_flow(pred_df)\n",
    "    UE_path_flow = get_all_path_flow(path_link_df)\n",
    "    path_flow = pd.merge(UE_path_flow, pred_path_flow, on='path', how='left')\n",
    "    path_flow = path_flow.rename(columns={'flow_x': 'UE_flow', 'flow_y': 'pred_flow'})\n",
    "    path_flow['abs_err'] = (path_flow['pred_flow'] - path_flow['UE_flow']).abs()\n",
    "    path_flow['sqr_err'] = path_flow['abs_err']**2\n",
    "    path_flow = path_flow[~path_flow['abs_err'].isna()]\n",
    "    return path_flow\n",
    "\n",
    "def calculate_indicator(flowList):\n",
    "    mse = np.mean([np.mean(flowList[x]['sqr_err']) for x in range(len(flowList))])\n",
    "    mae = np.mean([np.mean(flowList[x]['abs_err']) for x in range(len(flowList))])\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = [df['abs_err'][df['UE_flow']!=0]/ df['UE_flow'][df['UE_flow']!=0] for df in flowList]\n",
    "    mape = np.mean([j for i in mape for j in i])*100\n",
    "    return [round(mae,2), round(rmse,2), round(mape,2)]\n",
    "\n",
    "def single_avg_delay(pred_tensor, filename):\n",
    "    \"\"\" len_origin: number of OD pair in origin dataset\n",
    "    len_pred: number of OD pair in predicted value\n",
    "    nan_num: number of nan value\n",
    "    \"\"\"\n",
    "    stat = read_file(filename)\n",
    "    pred_df, len_origin, len_pred, nan_num = create_pred_df(pred_tensor, stat)\n",
    "    pred_link_flow = sum_pred_link_flow(pred_df, stat)\n",
    "    # Avg delay of predicted flow\n",
    "    pred_df, pred_avg_delay = calculate_delay(pred_df, pred_link_flow)\n",
    "    # Avg delay of solution\n",
    "    UE_link, path_link_df, avg_path_cost = mean_path_cost(stat)\n",
    "    a, solution_avg_delay = calculate_delay(path_link_df, UE_link)\n",
    "\n",
    "    link_flow = compare_link_flow(UE_link, pred_link_flow)\n",
    "    path_flow = compare_path_flow(path_link_df, pred_df)\n",
    "    return [link_flow, path_flow],[pred_avg_delay, solution_avg_delay], [len_pred, len_origin], nan_num, avg_path_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 4000\n",
    "FOLDERS = 'Solution/SiouxFalls/Output1'\n",
    "# FOLDERS = ['Solution/SiouxFalls/Output2/5by5_Data'] \n",
    "TRAIN_RATE = 0.7\n",
    "VAL_RATE = 0.2\n",
    "TEST_RATE = 0.1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# TRAINING \n",
    "device = 'gpu'\n",
    "input_dim = 6\n",
    "output_dim = 3\n",
    "d_model = 128\n",
    "heads=8\n",
    "E_layer = 8\n",
    "D_layer = 2\n",
    "epochs = 400\n",
    "learning_rate = 0.001\n",
    "dropout=0.2\n",
    "l2_reg=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path set number:  3864\n"
     ]
    }
   ],
   "source": [
    "# files = load_files_from_folders(FOLDERS, max_files=100)\n",
    "# path_set_dict = path_encoder(files)\n",
    "path_set_dict = read_file('Generate_data/SiouxFalls/unique_paths.pkl')\n",
    "print(\"Path set number: \", len(path_set_dict))\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, files):\n",
    "        # self.path_encoded = path_encoder()  # Get path encode dictionary\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "\n",
    "        for file_name in tqdm(files):\n",
    "            x, y = generate_xy(file_name, path_set_dict)\n",
    "            self.X.append(x)\n",
    "            self.Y.append(y)\n",
    "\n",
    "        self.X = tf.stack(self.X, axis=0)\n",
    "        self.Y = tf.stack(self.Y, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "    def to_tf_dataset(self, batch_size):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.X, self.Y))\n",
    "        dataset = dataset.shuffle(buffer_size=len(self.X)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "def get_test_set(files):\n",
    "    X = []\n",
    "    Y = []\n",
    "    Scalers = []\n",
    "    for file_name in tqdm(files) :\n",
    "        x, y, scaler = generate_xy(file_name, path_set_dict, test_set=True)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        Scalers.append(scaler)\n",
    "\n",
    "    X = tf.stack(X, axis=0)\n",
    "    Y = tf.stack(Y, axis=0)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset, Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30df5d509de42d9afaf16cd1374bf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755e0cc5af624d64b8eec01942f0c90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/799 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = load_files_from_folders(FOLDERS, max_files=DATA_SIZE)\n",
    "train_files, val_files, test_files1 = split_dataset(files, TRAIN_RATE, VAL_RATE)\n",
    "\n",
    "train_dataset = Dataset(train_files)\n",
    "train_data_loader = train_dataset.to_tf_dataset(BATCH_SIZE)\n",
    "\n",
    "val_dataset = Dataset(val_files)\n",
    "val_data_loader = train_dataset.to_tf_dataset(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc48ff714013473b86a57115bb323669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"multi_head_attention_24\" \"                 f\"(type MultiHeadAttention).\n\n{{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[32,8,576,576] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Einsum]\n\nCall arguments received by layer \"multi_head_attention_24\" \"                 f\"(type MultiHeadAttention):\n  • query=tf.Tensor(shape=(32, 576, 6), dtype=float16)\n  • value=tf.Tensor(shape=(32, 576, 6), dtype=float16)\n  • key=tf.Tensor(shape=(32, 576, 6), dtype=float16)\n  • attention_mask=None\n  • return_attention_scores=False\n  • training=True\n  • use_causal_mask=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(input_dim\u001b[38;5;241m=\u001b[39minput_dim, output_dim\u001b[38;5;241m=\u001b[39moutput_dim, \n\u001b[0;32m      6\u001b[0m                         d_model\u001b[38;5;241m=\u001b[39md_model,\n\u001b[0;32m      7\u001b[0m                         N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, heads\u001b[38;5;241m=\u001b[39mheads, \n\u001b[0;32m      8\u001b[0m                         dropout\u001b[38;5;241m=\u001b[39mdropout)\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 11\u001b[0m trained_model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 118\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, train_data_loader, val_data_loader, epochs, loss_fn, optimizer, device, gradient_accumulation_steps)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m--> 118\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(trg, output)\n\u001b[0;32m    120\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps  \u001b[38;5;66;03m# Normalize the loss to account for gradient accumulation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vu Tuan Minh\\miniconda3\\envs\\test_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[7], line 88\u001b[0m, in \u001b[0;36mTransformer.call\u001b[1;34m(self, x, y, training)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 88\u001b[0m     encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(y, encoder_output, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m     90\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivate(decoder_output)\n",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mEncoder.call\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mEncoderLayer.call\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output), training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m     27\u001b[0m     ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense1(x)), training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"multi_head_attention_24\" \"                 f\"(type MultiHeadAttention).\n\n{{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[32,8,576,576] and type half on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Einsum]\n\nCall arguments received by layer \"multi_head_attention_24\" \"                 f\"(type MultiHeadAttention):\n  • query=tf.Tensor(shape=(32, 576, 6), dtype=float16)\n  • value=tf.Tensor(shape=(32, 576, 6), dtype=float16)\n  • key=tf.Tensor(shape=(32, 576, 6), dtype=float16)\n  • attention_mask=None\n  • return_attention_scores=False\n  • training=True\n  • use_causal_mask=False"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "loss_fn = MeanSquaredError()\n",
    "model = Transformer(input_dim=input_dim, output_dim=output_dim, \n",
    "                        d_model=d_model,\n",
    "                        N=8, heads=heads, \n",
    "                        dropout=dropout)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "trained_model, train_losses, val_losses = training_loop(model, train_data_loader, val_data_loader, epochs, loss_fn, optimizer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
